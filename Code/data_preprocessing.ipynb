{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras.applications.nasnet as nasnet\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input\n",
    "from preprocess_data import PreprocessData\n",
    "\n",
    "def extract_frame_features(frame):\n",
    "    img = image.img_to_array(frame)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    base_model = nasnet.NASNetLarge(weights='imagenet')\n",
    "    features = base_model.predict(img)\n",
    "\n",
    "    return features.flatten()\n",
    "\n",
    "\n",
    "def preprocess_data(input_json_file, output_path, frame_width, frame_height):\n",
    "    \n",
    "    output_dir = os.path.join(output_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    dp = PreprocessData()\n",
    "\n",
    "    # Read the JSON file\n",
    "    with open(input_json_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    for file in data:\n",
    "        video_path = file['SENTENCE_FILE_PATH']\n",
    "        \n",
    "        video_name = os.path.split(video_path)[-1]\n",
    "        output_file = os.path.join(output_dir, video_name)\n",
    "\n",
    "        video_features = []\n",
    "\n",
    "        # Open the input video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        current_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        current_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # Define the codec for the output video\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        output = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height), isColor=False)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            resized_frame = dp.resize(frame, frame_width, frame_height)\n",
    "            grey_scaled_frame = dp.grey_scale(resized_frame)\n",
    "            noise_reduced_frame = dp.reduce_noise(grey_scaled_frame, 10, 7, 21)\n",
    "            # segmented_frame = dp.segment(noise_reduced_frame)\n",
    "\n",
    "            # features = extract_frame_features(resized_frame)\n",
    "            # video_features.append(features)\n",
    "            # video_features = np.stack(video_features)\n",
    "            # print(video_features.shape)\n",
    "\n",
    "            # Write the resized frame to the output video\n",
    "            output.write(segmented_frame)\n",
    "\n",
    "        # Release the video capture and writer objects\n",
    "        cap.release()\n",
    "        output.release()\n",
    "\n",
    "\n",
    "input_json_file = '../../Dataset/test/test.json'\n",
    "output_path = '../../Dataset/test/Segmented/'\n",
    "preprocess_data(input_json_file, output_path, 331, 331)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@239.625] global net_impl.cpp:1164 getLayerShapesRecursively OPENCV/DNN: [Eltwise]:(FeatureExtractor/MnasFPN/cell_0/block_0/add): getMemoryShapes() throws exception. inputs=2 outputs=1/1 blobs=0\n",
      "[ERROR:0@239.625] global net_impl.cpp:1167 getLayerShapesRecursively     input[0] = [ 1 256 21 21 ]\n",
      "[ERROR:0@239.625] global net_impl.cpp:1167 getLayerShapesRecursively     input[1] = [ 1 256 20 20 ]\n",
      "[ERROR:0@239.625] global net_impl.cpp:1171 getLayerShapesRecursively     output[0] = [ 1 256 21 21 ]\n",
      "[ERROR:0@239.625] global net_impl.cpp:1177 getLayerShapesRecursively Exception message: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/layers/eltwise_layer.cpp:258: error: (-215:Assertion failed) inputs[vecIdx][j] == inputs[i][j] in function 'getMemoryShapes'\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/layers/eltwise_layer.cpp:258: error: (-215:Assertion failed) inputs[vecIdx][j] == inputs[i][j] in function 'getMemoryShapes'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     video\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m     61\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m---> 63\u001b[0m detect_objects(\u001b[39m'\u001b[39;49m\u001b[39m../../Dataset/test/videos/_fZbAxSSbX4_0-5-rgb_front.mp4\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18/frozen_inference_graph.pb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mdetect_objects\u001b[0;34m(video_path, object_model_path, confidence_threshold)\u001b[0m\n\u001b[1;32m     32\u001b[0m net\u001b[39m.\u001b[39msetInput(blob)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Perform object detection\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m detections \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mforward()\n\u001b[1;32m     37\u001b[0m \u001b[39m# Process the detections\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(detections\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]):\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /Users/xperience/GHA-OCV-Python/_work/opencv-python/opencv-python/opencv/modules/dnn/src/layers/eltwise_layer.cpp:258: error: (-215:Assertion failed) inputs[vecIdx][j] == inputs[i][j] in function 'getMemoryShapes'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_objects(video_path, object_model_path, confidence_threshold=0.5):\n",
    "    # Load the pre-trained object detection model\n",
    "    net = cv2.dnn.readNetFromTensorflow(object_model_path)\n",
    "\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Define the object classes (labels) supported by the model\n",
    "    object_classes = {\n",
    "        1: 'person',\n",
    "        2: 'bicycle',\n",
    "        3: 'car',\n",
    "        # Add more object classes as per your requirements\n",
    "    }\n",
    "\n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize the frame to a suitable size for object detection\n",
    "        resized_frame = cv2.resize(frame, (331, 331))\n",
    "\n",
    "        # Prepare the input blob for the object detection model\n",
    "        blob = cv2.dnn.blobFromImage(resized_frame, 1.0, (331, 331), (127.5, 127.5, 127.5), swapRB=True, crop=False)\n",
    "\n",
    "        # Set the input blob as the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Perform object detection\n",
    "        detections = net.forward()\n",
    "\n",
    "        # Process the detections\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # Filter out weak detections based on the confidence threshold\n",
    "            if confidence > confidence_threshold:\n",
    "                class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "                # Get the bounding box coordinates\n",
    "                box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "                x, y, w, h = box.astype(int)\n",
    "\n",
    "                # Draw the bounding box and label on the frame\n",
    "                label = object_classes[class_id]\n",
    "                cv2.rectangle(frame, (x, y), (w, h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame with object detections\n",
    "        cv2.imshow(\"Object Detection\", frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "detect_objects('../../Dataset/test/videos/_fZbAxSSbX4_0-5-rgb_front.mp4', './ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18/frozen_inference_graph.pb')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
