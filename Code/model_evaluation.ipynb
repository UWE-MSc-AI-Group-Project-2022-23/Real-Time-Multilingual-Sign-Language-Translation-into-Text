{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(\"Loading trained seq2seq model...\")\n",
    "model = load_model('seq2seq_model.h5')\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Load max_seq_length\n",
    "with open('max_seq_length.pkl', 'rb') as f:\n",
    "        max_seq_length = pickle.load(f)\n",
    "\n",
    "print(\"Loading preprocessed sentences...\")\n",
    "preprocessed_file_path = './validation/preprocessed_sentences.json'\n",
    "with open(preprocessed_file_path, 'r') as f:\n",
    "    preprocessed_data = json.load(f)\n",
    "\n",
    "# Extract the encoder part of the model\n",
    "print(\"Extracting encoder from model...\")\n",
    "encoder_input_layer = model.get_layer('input_1').input\n",
    "encoder_output_layer, encoder_state_h, encoder_state_c = model.get_layer('lstm').output\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "encoder_model = Model(inputs=encoder_input_layer, outputs=encoder_states)\n",
    "\n",
    "# Build the decoder model\n",
    "print(\"Building decoder model...\")\n",
    "decoder_input = Input(shape=(None, len(tokenizer.word_index) + 1), name='decoder_input')\n",
    "decoder_state_input_h = Input(shape=(128,), name='decoder_state_input_h')\n",
    "decoder_state_input_c = Input(shape=(128,), name='decoder_state_input_c')\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.get_layer('lstm_1')\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_input, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.get_layer('dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    print(\".predict completed\")\n",
    "    target_seq = np.zeros((1, 1, len(tokenizer.word_index) + 1))\n",
    "    target_seq[0, 0, tokenizer.word_index['<START>']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    iteration_count = 0\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Printing the output tokens for the current step\n",
    "        print(f\"[DEBUG] Iteration {iteration_count}, Output Tokens: {output_tokens}\")\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = tokenizer.index_word.get(sampled_token_index)\n",
    "\n",
    "        # Printing the selected word for the current step\n",
    "        print(f\"[DEBUG] Iteration {iteration_count}, Sampled Word: {sampled_char}\")\n",
    "\n",
    "        if sampled_char:\n",
    "            decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        if sampled_char == '<END>' or len(decoded_sentence.split()) > max_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, len(tokenizer.word_index) + 1))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "\n",
    "        iteration_count += 1\n",
    "\n",
    "    # Printing the final decoded sentence\n",
    "    print(f\"[DEBUG] Decoded Sentence: {decoded_sentence}\")\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "# Load video features from the directory\n",
    "feature_dir = './validation/NASNetLarge_feature_vectors'\n",
    "print(\"[INFO] Video features directory set.\")\n",
    "\n",
    "video_features = []\n",
    "for item in preprocessed_data[:2]:\n",
    "    file_path = os.path.join(feature_dir, f\"{item['SENTENCE_NAME']}.json\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            video_features.append(json.load(f))\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load: {file_path}. Error: {e}\")\n",
    "\n",
    "print(\"[INFO] Loaded video features.\")\n",
    "\n",
    "# Calculate BLEU scores\n",
    "print(\"[INFO] Calculating BLEU scores...\")\n",
    "bleu_scores = []\n",
    "original_sentences = [item['SENTENCE_DESCRIPTION'] for item in preprocessed_data[:2]]\n",
    "\n",
    "if not video_features: \n",
    "    print(\"[ERROR] No video features loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "for i, features in enumerate(video_features):\n",
    "    features = np.array(features)  \n",
    "    input_seq = pad_sequences([features], maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    reference = [original_sentences[i].split()]\n",
    "    candidate = decoded_sentence.split()\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    bleu_scores.append(score)\n",
    "    print(f\"[INFO] Sentence {i+1}, BLEU Score: {score}\")\n",
    "\n",
    "print(f\"[INFO] Average BLEU Score: {np.mean(bleu_scores)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
